# -*- coding: utf-8 -*-
"""Test_GaitGL_CASIA-B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z0HTGeTgejviPnFG5WVYhKcZbLCKEXhN
"""

import gdown
url = 'https://drive.google.com/file/d/15Ixkun8I38gsZc9nM2C9nrMFy_dQbQUd/view?usp=drive_link'
output_path = 'CASIA-B.zip'
gdown.download(url, output_path, quiet=False,fuzzy=True)

!unzip -qq CASIA-B.zip

# ============================================================
# 1. MONTAR DRIVE (opcional) PARA GUARDAR CHECKPOINTS
# ============================================================
import os

USE_DRIVE = True  # pon False si no quieres montar Drive

OUT_DIR = "/content/gaitgl_runs"
if USE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')
    OUT_DIR = "/content/drive/MyDrive/Tesis Maestría/Desarrollo/gaitgl-runs"

os.makedirs(OUT_DIR, exist_ok=True)
print("OUT_DIR:", OUT_DIR)

!rm -rf /content/GaitGL
!git clone https://github.com/bb12346/GaitGL.git /content/GaitGL

import torch, torchvision, sys
print("Torch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())

sys.path.append("/content/GaitGL")

# ============================================================
# 3. PREPROCESAMIENTO CASIA-B → FORMATO GAITGL
#
# Qué queremos:
#  - Reorganizar las máscaras/siluetas de CASIA-B en carpetas:
#       <root>/<ID>/<cond>/<view>/<seq>/*.png
#    ó al menos <root>/<ID>/<cond>/<view>/*.png
#    donde cada PNG es 1 silueta binaria (64x64 idealmente).
#
#  - Redimensionar/recortar cada silueta para que esté alineada
#    tipo 64x64 como dice GaitGL para OU-MVLP (usan pretreatment_oumvlp.py
#    para alinear sujeto al centro y normalizar tamaño). :contentReference[oaicite:7]{index=7}
#
#  - Ignorar sujeto '005' porque está incompleto en CASIA-B, según config
#    típica de GaitSet. :contentReference[oaicite:8]{index=8}
#
# Asumimos que tu zip descomprimió siluetas binarias en /content/output
# con nombres estilo "001-nm-01-090-001.png" etc. (tu loader anterior
# soportaba ese patrón). Si tu estructura es distinta, ajusta
# la función list_casia_sequences() más abajo.
# ============================================================

import cv2
import glob
import re
import numpy as np
from pathlib import Path
import shutil

CASIA_RAW_ROOT = Path("/content/output")     # <- ajusta si tu zip se descomprime distinto
CASIA_GL_ROOT  = Path("/content/CASIA_B_GaitGL")  # destino para GaitGL/GaitSet-style
CASIA_GL_ROOT.mkdir(parents=True, exist_ok=True)

file_re = re.compile(r"^(\d+)-(nm|bg|cl)-(\d+)-(\d+)-(\d+)\.png$", re.IGNORECASE)
# grupos:
#   pid, cond, seq, view, frameIdx

def preprocess_and_copy_to_gaitgl(src_root, dst_root, size=64):
    """
    Lee todas las siluetas de CASIA-B ya segmentadas,
    las centra dentro de un bbox apretado, las normaliza a size x size (64x64),
    y las copia a la carpeta destino con jerarquía ID/cond/view/seq/*.png
    """
    pngs = list(src_root.rglob("*.png"))
    print("Total PNGs encontrados:", len(pngs))
    for p in pngs:
        fname = p.name
        m = file_re.match(fname)
        if not m:
            # si no calza el patrón, intenta fallback con split("-")
            parts = fname.split("-")
            if len(parts) < 5:
                continue
            pid, cond, seq, view = parts[0], parts[1], parts[2], parts[3]
            frame_id = parts[4].split(".")[0]
        else:
            pid, cond, seq, view, frame_id = m.groups()

        # Salta el sujeto 005 (incompleto en CASIA-B). :contentReference[oaicite:9]{index=9}
        if pid == "005":
            continue

        # lee máscara
        mask = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            continue

        # binariza
        _,mask_bin = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)

        # recorta bbox apretado alrededor del sujeto (similar idea a pretreatment)
        ys, xs = np.where(mask_bin > 0)
        if len(xs)==0 or len(ys)==0:
            continue
        x_min, x_max = xs.min(), xs.max()
        y_min, y_max = ys.min(), ys.max()
        crop = mask_bin[y_min:y_max+1, x_min:x_max+1]

        # redimensiona cuadrado size x size (64x64). GaitGL requiere 64x64
        # después de pretreatment_oumvlp.py. :contentReference[oaicite:10]{index=10}
        crop_resized = cv2.resize(crop, (size, size), interpolation=cv2.INTER_NEAREST)

        # crea carpeta destino
        # usaremos: dst_root/pid/cond/view/seq/
        out_dir = dst_root / pid / cond / view / seq
        out_dir.mkdir(parents=True, exist_ok=True)

        out_path = out_dir / f"{pid}-{cond}-{seq}-{view}-{frame_id}.png"
        cv2.imwrite(str(out_path), crop_resized)

preprocess_and_copy_to_gaitgl(CASIA_RAW_ROOT, CASIA_GL_ROOT, size=64)

print("Estructura ejemplo:")
!find /content/CASIA_B_GaitGL -maxdepth 4 -type d | head

# ============================================================
# 4. ESCRIBIR config.py ADAPTADA A CASIA-B
#
# El script train.py de GaitGL hace:
#   from model.initialization import initialization
#   from config import conf
#   ...
#   m = initialization(conf, train=opt.cache)[0]
#   m.fit()
#
# Así que sólo tenemos que sobrescribir /content/GaitGL/config.py
# con un dict 'conf' similar a GaitSet pero apuntando a CASIA-B.
#
# Campos importantes (basado en config.py de GaitSet):  :contentReference[oaicite:11]{index=11}
#   WORK_PATH: carpeta de trabajo (logs, checkpoints)
#   CUDA_VISIBLE_DEVICES: "0" en Colab
#   data: {
#       'dataset_path': ruta preprocess (CASIA_GL_ROOT),
#       'resolution': '64',
#       'dataset': 'CASIA-B',
#       'pid_num': 73,            # sujetos train (sin #005, etc.)
#       'pid_shuffle': False
#   }
#   model: {
#       'hidden_dim': 256,
#       'lr': 1e-4,
#       'hard_or_full_trip': 'full',
#       'batch_size': (8,16),     # P x K
#       'restore_iter': 0,
#       'total_iter': 80000,      # puedes subirlo luego
#       'margin': 0.2,
#       'num_workers': 2,
#       'frame_num': 30,          # #frames por set
#       'model_name': 'GaitGL'    # nombre para logs
#   }
#
# NOTA: GaitGL README sugiere batch_size (32,8) y total_iter 250000 para OU-MVLP,
# y otra config para CASIA-E con frame_num=64. :contentReference[oaicite:12]{index=12}
# Para CASIA-B empezamos más chiquito (8,16 / 80k iters) para que quepa en Colab.
# ============================================================

import os
from pathlib import Path

WORK_PATH = "/content/GaitGL/work"
os.makedirs(WORK_PATH, exist_ok=True)

CASIA_GL_ROOT = "/content/CASIA_B_GaitGL"  # ruta preprocess 64x64 que construimos antes

conf_casiab = f"""
conf = {{
    "WORK_PATH": "{WORK_PATH}",
    "CUDA_VISIBLE_DEVICES": "0",
    "data": {{
        "dataset_path": "{CASIA_GL_ROOT}",
        "resolution": "64",
        "dataset": "CASIA-B",
        "pid_num": 73,
        "pid_shuffle": False
    }},
    "model": {{
        "hidden_dim": 256,
        "lr": 1e-4,
        "hard_or_full_trip": "full",
        "batch_size": (8,16),
        "restore_iter": 0,
        "total_iter": 10000,
        "margin": 0.2,
        "num_workers": 2,
        "frame_num": 30,
        "model_name": "GaitGL"
    }}
}}
"""

with open("/content/GaitGL/config.py", "w") as f:
    f.write(conf_casiab)

print("Nuevo config.py escrito ✅")
print(conf_casiab)

from pathlib import Path
import textwrap

patched_code = textwrap.dedent(r"""
import os
import os.path as osp
import numpy as np

from .data_set import DataSet


def load_data(dataset_path, resolution, dataset, pid_num, pid_shuffle, cache=True):

    seq_dir = []
    view = []
    seq_type = []
    label = []

    # ---- 1. Recolectar secuencias ----
    # dataset_path/<pid>/<cond>/<view>/<seq>/*.png
    for _label in sorted(list(os.listdir(dataset_path))):
        label_path = osp.join(dataset_path, _label)
        if not osp.isdir(label_path):
            continue

        for _seq_type in sorted(list(os.listdir(label_path))):
            seq_type_path = osp.join(label_path, _seq_type)
            if not osp.isdir(seq_type_path):
                continue

            for _view in sorted(list(os.listdir(seq_type_path))):
                view_path = osp.join(seq_type_path, _view)
                if not osp.isdir(view_path):
                    continue

                # Dentro de view_path puede haber:
                #   A) subcarpetas de secuencia
                #   B) directamente PNGs
                subdirs = [d for d in sorted(os.listdir(view_path))
                           if osp.isdir(osp.join(view_path, d))]

                if len(subdirs) > 0:
                    # Caso A: múltiples secuencias dentro de <view>
                    for _seq in subdirs:
                        _seq_dir = osp.join(view_path, _seq)
                        # ¿tiene frames?
                        frames = [f for f in os.listdir(_seq_dir)
                                  if f.lower().endswith(".png")]
                        if len(frames) == 0:
                            continue
                        # IMPORTANTE: el código original guarda cada secuencia
                        # como una LISTA con UNA ruta base [ruta_seq]
                        seq_dir.append([_seq_dir])
                        label.append(_label)
                        seq_type.append(_seq_type)
                        view.append(_view)
                else:
                    # Caso B: PNGs directo en <view>
                    frames = [f for f in os.listdir(view_path)
                              if f.lower().endswith(".png")]
                    if len(frames) == 0:
                        continue
                    seq_dir.append([view_path])
                    label.append(_label)
                    seq_type.append(_seq_type)
                    view.append(_view)

    # ---- 2. Generar / cargar split train/test por IDs ----
    os.makedirs('partition', exist_ok=True)
    pid_fname = osp.join('partition', '{}_{}_{}.npy'.format(dataset, pid_num, pid_shuffle))

    # lista única de IDs presentes realmente
    pid_all = sorted(list(set(label)))

    # Si pid_num declarado en config es mayor que los IDs reales, lo recortamos
    real_pid_num = min(pid_num, len(pid_all))

    if (dataset in ('CASIA-B', 'CASIA-E')):
        # branch CASIA
        if not osp.exists(pid_fname):
            # ordenamos IDs
            pid_list_unique = pid_all[:]
            if pid_shuffle:
                np.random.shuffle(pid_list_unique)

            # split [0:pid_num] -> train, resto -> test
            train_ids = pid_list_unique[0:real_pid_num]
            test_ids  = pid_list_unique[real_pid_num:]

            # guardamos como arreglo objeto para NumPy 2.x
            pid_list_to_save = [train_ids, test_ids]
            pid_arr = np.array(pid_list_to_save, dtype=object)
            np.save(pid_fname, pid_arr, allow_pickle=True)
        # cargamos siempre con allow_pickle
        pid_arr_loaded = np.load(pid_fname, allow_pickle=True)
        train_list = list(pid_arr_loaded[0])
        test_list  = list(pid_arr_loaded[1])
    else:
        # branch OU-MVLP (10k IDs en par/impar)
        if not osp.exists(pid_fname):
            pid_list = []
            # primero impares, luego pares, luego '10307'
            for i in range(1,10307):
                if i % 2 != 0:
                    pid_list.append('%05d' % i)
            for i in range(1,10307):
                if i % 2 == 0:
                    pid_list.append('%05d' % i)
            pid_list.append('10307')

            if pid_shuffle:
                np.random.shuffle(pid_list)

            train_ids = pid_list[0:real_pid_num]
            test_ids  = pid_list[real_pid_num:]

            pid_list_to_save = [train_ids, test_ids]
            pid_arr = np.array(pid_list_to_save, dtype=object)
            np.save(pid_fname, pid_arr, allow_pickle=True)

        pid_arr_loaded = np.load(pid_fname, allow_pickle=True)
        train_list = list(pid_arr_loaded[0])
        test_list  = list(pid_arr_loaded[1])

    print('lentestdata--', len(train_list), len(test_list))

    # ---- 3. Construir DataSet de train y test ----
    train_source = DataSet(
        [seq_dir[i]    for i, l in enumerate(label) if l in train_list],
        [label[i]      for i, l in enumerate(label) if l in train_list],
        [seq_type[i]   for i, l in enumerate(label) if l in train_list],
        [view[i]       for i, l in enumerate(label) if l in train_list],
        cache, resolution, cut=True
    )

    test_source = DataSet(
        [seq_dir[i]    for i, l in enumerate(label) if l in test_list],
        [label[i]      for i, l in enumerate(label) if l in test_list],
        [seq_type[i]   for i, l in enumerate(label) if l in test_list],
        [view[i]       for i, l in enumerate(label) if l in test_list],
        cache, resolution, cut=True
    )

    print('len train,test--', len(train_source), len(test_source))
    return train_source, test_source
""")

dl_path = Path("/content/GaitGL/model/utils/data_loader.py")
dl_path.write_text(patched_code)

print("✅ data_loader.py reescrito con el parche final compatible con tu estructura y DataSet()")

import re
from pathlib import Path

model_py = Path("/content/GaitGL/model/model.py")
src = model_py.read_text()

# 1. Import torch si no está importado arriba en ese archivo.
#    (por seguridad, lo forzamos a estar)
if "import torch" not in src.splitlines()[0:20]:
    src = "import torch\n" + src

# 2. Reemplazar cualquier DataParallel([...0,1,2,3]) o variantes
pattern_dp = re.compile(
    r"nn\.DataParallel\(\s*([^,]+)\s*,\s*device_ids=\[[^\]]+\]\s*\)"
)

def repl(match):
    inner = match.group(1).strip()
    # envolvemos en un bloque condicional para 1-GPU
    return (
        "nn.DataParallel("
        + inner
        + ", device_ids=[0] if torch.cuda.is_available() else None)"
    )

new_src = re.sub(pattern_dp, repl, src)

model_py.write_text(new_src)

print("✅ model/model.py parcheado para usar solo GPU 0 en DataParallel")

from pathlib import Path
import textwrap

triplet_path = Path("/content/GaitGL/model/network/triplet.py")

patched_triplet = textwrap.dedent(r"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class TripletLoss(nn.Module):
    def __init__(self, batch_size, hard_or_full, margin):
        super(TripletLoss, self).__init__()
        self.batch_size = batch_size
        self.margin = margin
        # Nota: hard_or_full está en el init original,
        # pero en este forward se computan ambas variantes igual.

    def forward(self, feature, label):

        n, m, d = feature.size()

        # Máscaras de positivos / negativos.
        # En PyTorch>=1.2 ya no se usa .byte() para máscaras en masked_select,
        # tienen que ser bool.
        hp_mask = (label.unsqueeze(1) == label.unsqueeze(2)).bool().view(-1)  # positivos
        hn_mask = (label.unsqueeze(1) != label.unsqueeze(2)).bool().view(-1)  # negativos

        # distancias pairwise dentro del batch
        dist = self.batch_dist(feature)        # [n, m, m]
        mean_dist = dist.mean(1).mean(1)       # distancia promedio por identidad
        dist = dist.view(-1)                   # [n*m*m]

        # ----- hard mining -----
        # Para cada ancla, tomar el positivo más lejano y el negativo más cercano
        hp_sel = torch.masked_select(dist, hp_mask)  # todos los pares positivos
        hn_sel = torch.masked_select(dist, hn_mask)  # todos los pares negativos

        # reshape a [n, m, -1] para agrupar por identidad y por muestra
        hard_hp_dist = torch.max(hp_sel.view(n, m, -1), 2)[0]
        hard_hn_dist = torch.min(hn_sel.view(n, m, -1), 2)[0]
        hard_loss_metric = F.relu(self.margin + hard_hp_dist - hard_hn_dist).view(n, -1)
        hard_loss_metric_mean = torch.mean(hard_loss_metric, 1)

        # ----- full (todas las combinaciones válidas) -----
        full_hp_dist = hp_sel.view(n, m, -1, 1)
        full_hn_dist = hn_sel.view(n, m, 1, -1)
        full_loss_metric = F.relu(self.margin + full_hp_dist - full_hn_dist).view(n, -1)

        full_loss_metric_sum = full_loss_metric.sum(1)
        full_loss_num = (full_loss_metric != 0).sum(1).float()

        full_loss_metric_mean = full_loss_metric_sum / full_loss_num
        full_loss_metric_mean[full_loss_num == 0] = 0

        return full_loss_metric_mean, hard_loss_metric_mean, mean_dist, full_loss_num

    def batch_dist(self, x):
        # ||x||^2
        x2 = torch.sum(x ** 2, 2)  # [n, m]
        # dist(a,b)^2 = ||a||^2 + ||b||^2 - 2 a·b
        dist2 = x2.unsqueeze(2) + x2.unsqueeze(1) - 2 * torch.matmul(x, x.transpose(1, 2))
        dist2 = F.relu(dist2)  # clamp numérico
        dist = torch.sqrt(dist2 + 1e-12)
        return dist
""")

triplet_path.write_text(patched_triplet)

print("✅ triplet.py parcheado para máscaras bool y compatibilidad con PyTorch moderno")

!python /content/GaitGL/train.py --cache TRUE

# ============================================================
# 6. GUARDAR CHECKPOINTS AL DRIVE
#
# GaitGL (como GaitSet) va guardando pesos durante el training en WORK_PATH,
# normalmente algo tipo work/checkpoint/<iter>.pth o similar.
# Si quieres copiar todo a OUT_DIR (Drive), hazlo:
# ============================================================

import shutil, os, datetime

timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
backup_dir = os.path.join(OUT_DIR, f"ckpts_{timestamp}")
os.makedirs(backup_dir, exist_ok=True)

shutil.copytree("/content/GaitGL/work", backup_dir, dirs_exist_ok=True)
print("Checkpoints/backups copiados a:", backup_dir)

"""## INFERENCIA"""

import sys, os, torch
from pathlib import Path
from importlib import reload

device = "cuda" if torch.cuda.is_available() else "cpu"
sys.path.append("/content/GaitGL")

# 1. Cargar tu config
import config
reload(config)
conf = config.conf

# ⚠ IMPORTANTE:
# Tus checkpoints dicen `_128_full_30-10000-...`, lo que nos dice:
#   hidden_dim ~= 128
#   hard_or_full_trip = "full"
#   frame_num = 30
#   margin = 0.2
#   iter guardado = 10000
#
# Si en conf["model"]["hidden_dim"] tienes 256, cámbialo aquí:
conf["model"]["hidden_dim"] = 128

# 2. Valores que pide Model.__init__
#    Firma de Model.__init__:
#    def __init__(self,
#        hidden_dim, lr, hard_or_full_trip, margin, num_workers,
#        batch_size, restore_iter, total_iter, save_name, train_pid_num,
#        frame_num, model_name, train_source, test_source, img_size=112)
#
# Algunos vienen de conf directamente, otros los marcamos nosotros.
hidden_dim         = conf["model"]["hidden_dim"]
lr                 = conf["model"]["lr"]
hard_or_full_trip  = conf["model"]["hard_or_full_trip"]
margin             = conf["model"]["margin"]
num_workers        = conf["model"]["num_workers"]
batch_size         = conf["model"]["batch_size"]        # (P, M)
restore_iter       = 0                                  # para inferencia no reanudamos training
total_iter         = conf["model"]["total_iter"] if "total_iter" in conf["model"] else conf["model"].get("total_iter", conf["model"].get("total_iter", 80000))
save_name          = f"GaitGL_{conf['data']['dataset']}_{conf['data']['pid_num']}_{conf['data']['pid_shuffle']}_{margin}_{hidden_dim}_{hard_or_full_trip}_{conf['model']['frame_num']}"
train_pid_num      = conf["data"]["pid_num"]
frame_num          = conf["model"]["frame_num"]
model_name         = conf["model"]["model_name"]
img_size           = 112  # el constructor Model usa 112 por defecto

# Para inferencia directa NO necesitamos data loaders en el objeto,
# pero la clase Model exige train_source y test_source.
# Le damos mocks mínimos con atributos usados dentro de Model.transform().
# transform() usa:
#   self.test_source / self.train_source para:
#       - .label_set
#       - ser pasada a DataLoader con TripletSampler/collate_fn
#
# Nosotros NO vamos a usar transform() aquí, vamos a hacer inferencia manual,
# así que podemos pasarle placeholders simples.
class DummyDataset:
    def __len__(self): return 0
    label_set = []

dummy_train = DummyDataset()
dummy_test  = DummyDataset()

from model.model import Model
gait_model = Model(
    hidden_dim,
    lr,
    hard_or_full_trip,
    margin,
    num_workers,
    batch_size,
    restore_iter,
    total_iter,
    save_name,
    train_pid_num,
    frame_num,
    model_name,
    dummy_train,
    dummy_test,
    img_size
)

# movemos todo a GPU si hay
gait_model.m_resnet = gait_model.m_resnet.to(device)
gait_model.triplet_loss = gait_model.triplet_loss.to(device)

# 3. Cargar pesos del encoder que generaste al entrenar
ckpt_path = "/content/GaitGL/work/checkpoint/GaitGL/GaitGL_CASIA-B_73_False_0.2_128_full_30-10000-encoder.ptm"

state = torch.load(ckpt_path, map_location=device)
# En tu código de save(), haces:
# torch.save(self.m_resnet.state_dict(), <...>-encoder.ptm)
# o sea: el archivo ES un state_dict ya.
state_dict = state

# Cargamos ese state_dict SOLO en la subred m_resnet (porque es lo que guardaste)
missing, unexpected = gait_model.m_resnet.load_state_dict(state_dict, strict=False), None
print("Cargado checkpoint en m_resnet. Posibles missing/unexpected keys:", missing)

# Ponemos el modelo en eval
gait_model.m_resnet.eval()
print("Modelo listo en modo eval en", device)

import cv2
import numpy as np
import glob
import torch

def load_seq_folder_to_tensor(seq_dir, frame_num=30, img_size=112):
    """
    seq_dir: carpeta con siluetas .png (ya recortadas y alineadas, por ejemplo 64x64)
    frame_num: el frame_num del modelo (conf["model"]["frame_num"], ej 30)
    img_size: el tamaño espacial que espera la red. En tu Model, img_size=112.
              Si tus siluetas actuales son 64x64, las vamos a upsamplear a 112x112
              con nearest-neighbor para ser coherentes.
    Devuelve tensor shape [1, 1, T, img_size, img_size] listo para m_resnet.
    """

    paths = sorted(glob.glob(str(Path(seq_dir) / "*.png")))
    if len(paths) == 0:
        raise RuntimeError(f"No se encontraron PNGs en {seq_dir}")

    # muestrea exactamente frame_num frames (con repetición si hace falta)
    if len(paths) >= frame_num:
        idxs = np.linspace(0, len(paths)-1, num=frame_num, dtype=int)
    else:
        base = np.arange(len(paths))
        extra = np.random.choice(len(paths), frame_num - len(paths), replace=True)
        idxs = np.concatenate([base, extra])

    frames = []
    for i in idxs:
        m = cv2.imread(paths[i], cv2.IMREAD_GRAYSCALE)
        if m is None:
            # fallo de lectura -> frame negro
            m = np.zeros((img_size, img_size), np.uint8)

        # nuestras siluetas preprocesadas estaban en 64x64.
        # el modelo fue definido con img_size=112 por defecto,
        # así que las redimensionamos a 112x112 NEAREST para no introducir blur.
        m = cv2.resize(m, (img_size, img_size), interpolation=cv2.INTER_NEAREST)

        # binarizamos igual que en training (0/1 float32)
        m = (m > 127).astype(np.float32)

        # ahora m tiene shape [H,W] -> queremos [1,H,W]
        frames.append(m[None, ...])

    # stack -> [T,1,H,W]
    clip = np.stack(frames, axis=0)  # [T,1,H,W]

    # ahora queremos [B,1,T,H,W] con B=1
    # fíjate: en fit() después del collate hace seq = seq.unsqueeze(1)
    # donde seq era [batch, frames, H, W] -> [batch, 1, frames, H, W]
    # Aquí estamos en [T,1,H,W], así que reordenamos a [1,1,T,H,W]
    clip = np.transpose(clip, (1,0,2,3))  # [1,T,H,W]
    clip = clip[None, ...]               # [1,1,T,H,W]

    tens = torch.from_numpy(clip).float().to(device)
    return tens  # [1,1,T,H,W]

def extract_embedding_from_dir(gait_model, seq_dir, frame_num=30, img_size=112):
    """
    Devuelve un embedding numpy [D] para la secuencia en seq_dir.
    Usa la misma lógica que transform(), pero sólo para una carpeta.
    """
    gait_model.m_resnet.eval()

    # construye tensor [1,1,T,H,W]
    seq_tensor = load_seq_folder_to_tensor(seq_dir, frame_num=frame_num, img_size=img_size)

    with torch.no_grad():
        outputs, _ = gait_model.m_resnet(seq_tensor)
        # outputs shape: [n, c, ...] según la red; en transform() asumen [n, *, *]

        n = outputs.size(0)
        flat = outputs.view(n, -1)          # [n, D]
        meaned = torch.mean(flat, dim=0)    # [D]
        emb = meaned.cpu().numpy()          # [D]

    return emb

def list_sequence_dirs(root: Path, min_frames=5):
    """
    Devuelve una lista de dicts:
    {
        "pid": "001",
        "cond": "nm",
        "view": "090",
        "seq": "seq01",
        "path": Path(...)
    }
    para cada carpeta con suficientes PNGs.
    """
    out = []
    for pid_dir in sorted(root.iterdir()):
        if not pid_dir.is_dir():
            continue
        pid = pid_dir.name
        for cond_dir in sorted(pid_dir.iterdir()):
            if not cond_dir.is_dir():
                continue
            cond = cond_dir.name
            for view_dir in sorted(cond_dir.iterdir()):
                if not view_dir.is_dir():
                    continue
                view = view_dir.name
                subdirs = [d for d in sorted(view_dir.iterdir()) if d.is_dir()]
                if subdirs:
                    for seq_dir in subdirs:
                        frames = list(seq_dir.glob("*.png"))
                        if len(frames) >= min_frames:
                            out.append({
                                "pid": pid,
                                "cond": cond,
                                "view": view,
                                "seq": seq_dir.name,
                                "path": seq_dir
                            })
                else:
                    frames = list(view_dir.glob("*.png"))
                    if len(frames) >= min_frames:
                        out.append({
                            "pid": pid,
                            "cond": cond,
                            "view": view,
                            "seq": "seq0",
                            "path": view_dir
                        })
    return out

def cosine_sim(a, b):
    a = np.asarray(a)
    b = np.asarray(b)
    return float(np.dot(a,b) / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-8))

from pathlib import Path
import numpy as np

CASIA_GL_ROOT = Path("/content/CASIA_B_GaitGL")  # carpeta preprocess 64x64 que creaste

all_seqs = list_sequence_dirs(CASIA_GL_ROOT, min_frames=5)
print("Total secuencias detectadas:", len(all_seqs))
print("Ejemplo:", all_seqs[0])

# Elegimos una secuencia A
seq_a = all_seqs[0]

# Buscamos otra secuencia B:
# - idealmente misma persona (mismo pid) pero distinta vista / distinto seq
cands_same_pid = [s for s in all_seqs if s["pid"] == seq_a["pid"] and s["path"] != seq_a["path"]]
if len(cands_same_pid) > 0:
    seq_b = cands_same_pid[0]
else:
    # fallback: otra persona
    seq_b = all_seqs[1]

print("A:", seq_a)
print("B:", seq_b)

emb_a = extract_embedding_from_dir(
    gait_model,
    seq_a["path"],
    frame_num=frame_num,
    img_size=img_size
)

emb_b = extract_embedding_from_dir(
    gait_model,
    seq_b["path"],
    frame_num=frame_num,
    img_size=img_size
)

print("Embedding A dim:", emb_a.shape)
print("Embedding B dim:", emb_b.shape)

sim = cosine_sim(emb_a, emb_b)
print("Similitud coseno A vs B:", sim)
print(f"PID A={seq_a['pid']} vs PID B={seq_b['pid']}")
print(f"Vista A={seq_a['view']} vs Vista B={seq_b['view']}")

